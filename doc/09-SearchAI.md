## SearchAI：基于搜索实现的AI
### alpha-beta剪枝
* 讲个故事
一个父亲有两个儿子，大儿子有两个儿子，二儿子也有两个儿子。于是，父亲有四个孙子。
一天，大儿子看了一眼大孙子，说：“爹，您太伟大了，您伟大的功绩值100块钱！”父亲原来以为自己一生平庸一文不值，现在听到大儿子如此盛赞，不好意思的笑着说：“谬赞，谬赞！”过了片刻，大儿子又看了一眼二儿子，对父亲说：“确实有点谬赞，您的功绩也就值50块钱。”父亲听了心里不是滋味，想打大儿子一巴掌，结果想想自己“值100”这句话也是从大儿子嘴里冒出来的，也就罢休了。毕竟，一开始父亲以为自己一文不值，现在相信大儿子能让自己值50块钱，所以父亲也就相信大儿子了。现在，父亲承认了自己值50块钱。过了几天，二儿子看了一眼自己的大儿子，对父亲说：“爹，根据您的三孙子的长相能为，您也就值20块钱。”父亲听了勃然大怒，狠狠打了二儿子一巴掌，骂道：“他妈的，你大哥都说了，老子至少值50，老子宁愿相信你大哥也不能相信你！”二儿子吓傻了，本来还想根据自己的二儿子评估一下父亲，结果现在吓得浑身哆嗦，默默地使眼色给二儿子叫他躲远点，不用见爷爷了。  

* alpha-beta剪枝对每个结点维护一个可取值窗口。如果子孙结点的取值超出了这个窗口，子孙结点就会被剪枝。
* alpha-beta剪枝不一定全部实现，也可以写成alpha剪枝或者beta剪枝，那样效率会低一些。

beta剪枝的过程如下，fatherValue就是beta值，是父节点取值的下界，如果子孙结点的取值比这个下界还要小，就要对此节点进行剪枝。
```python
def go(state,fatherValue,depth):
	if depth==MAX_DEPTH:
		myValue=调用评估器评估state
		return myValue
	minSonValue=MAX_VALUE
	for sonState in getSons(state):
		sonValue=go(sonState,-minSonValue,depth+1)
		if sonValue<minSonValue:
			minSonValue=sonValue
		if minSonValue<=fatherValue:
			return MAX_VALUE
	return -minSonValue
```

alpha-beta剪枝有两种写法，方法一比较直观但略微繁琐，方法二写法简洁但不够直观。

方法一：
```
Function alpha_beta(node, depth, alpha, beta, player)
	If depth = 0 then
		return evaluate(node)
	if play = ‖black‖ then
		for each child of node do
			alpha = max(alpha, alpha_beta(child, depth-1, alpha, beta, ―White‖)
			if alpha >= beta then break
				return alpha
			else do
			for each child of node do
				beta = min(beta, alpha_beta(child, depth-1, alpha, beta, ―Black‖)
				if beta <= alpha then break
				return beta
/*开始执行*/
alpha_beta(root, depth, −∞, +∞, ―Black‖)
```
方法二：
```cpp
int AlphaBeta(int depth, int alpha, int beta) {
　if (depth == 0) {
　　return Evaluate();
　}
　GenerateLegalMoves();
　while (MovesLeft()) {
　　MakeNextMove();
　　val = -AlphaBeta(depth - 1, -beta, -alpha);
　　UnmakeMove();
　　if (val >= beta) {
　　　return beta;
　　}
　　if (val > alpha) {
　　　alpha = val;
　　}
　}
　return alpha;
}
```

### 局面评估
* 局面评估使用最简单的黑棋个数减去白棋个数表明当前棋局的估值。  
* 如果为必胜态，则估值为极大值（不是黑白之差）。 
* 如果为必败态，则估值为极小值（不是黑白之差）。 

### alpha-beta剪枝优化
要让父节点的值以最快的速度下降，这样能够增大剪枝力度。做法是优先深度优先搜索那些看上去比较好的着法而不是按照固定的顺序进行搜索。  
也就是说，生成全部的着法，对着法列表进行排序，优先搜索扩展那些比较好的局面。

### 一个问题
使用暴力搜索，至少看（）步才能保证自己立于不败之地。  
也就是说至少展开几层才能保证自己不败。  

#### 什么叫做立于不败之地？  
就是如果这个棋局不是必败局面，我给出的着法（也就是下一状态）就一定不会让神一样的对手看出破绽，我一定不会因为我的失误而失败。如果是必败局面，我完全不在乎，可以瞎走。而如果棋局是必胜局面，我不一定能看出来必胜。我是一个不求胜只求不败、不求有功但求无过的人。  
定义：立于不败之地
* 必胜局：我不输
* 和局：我不输
* 败局：我随意

立于不败之地的几种错误理解：   
对于任意给定的棋局，TableAI给这个棋局贴上“胜败和”三种标签，SearchAI也给这个棋局贴上一个标签，如果这两个标签不同，那么SearchAI没立在不败之地。
反例：SearchAI可能无法给一个胜局贴上必胜标签（因为搜索较浅，它无法捉住对手的破绽打败对手），但是SearchAI照样能够在这种错误认识下走出一招正确的着法。走了若干步之后，它可能恍然大悟自己必胜。 
也就是说：考量状态标签只是评价一个AI的观点，并不能代表这个AI的行动。
而另一方面，很多AI的着法有多种，AI返回的只是从可行着法中随机选取一种着法，所以无法考量一个AI针对某个局面的全部着法。也就是说，考量“观点”是错误的，考量“行动”是不可行的，难以枚举的。但是，对于本SearchAI每次返回的着法都是一个固定的列表，中间不存在随机因素。所以可以采用考量着法的方式判断AI是否达到无敌水平。  
错误理解二：对于给定的棋局，如果它是非必败局，那么SearchAI认为这局非必败。  
反例：当搜索深度非常浅的时候，SearchAI总是盲目乐观地认为自己非必败，因为它目光短浅，看不到自己黑暗的未来。  
正确理解：对于给定的棋局，如果它是非必败局（TableAI的观点），那么SearchAI给出的任意着法必定不会导致SearchAI走向失败，也就是说下一状态一定不会是对手的必胜状态（TableAI评估下一状态的观点）。

对于任何贪心规则，只要搜索足够深，必然有解。只是贪心规则有好有坏，好的贪心规则能够使搜索深度小，坏的贪心规则有跟没有区别不大。 
保证自己子力不落后，这就是一个贪心规则，这个规则能够使得搜索深度减小。 

### SearchAI把和棋当做必胜的情况
SearchAI有4子，敌人有3子时。本来是和棋状态，4子无法打败3子。SearchAI觉得最终和棋状态自己子多，所以觉得自己算赢，而实际上，不能算赢。  
方案一：返回0，和棋。这种方法在SearchAI搜索深度较小的情况下会走出大量昏招，因为它把许多必败状态当成了和棋。这样做肯定是不可以的，它会影响搜索的质量（包括剪枝可能会减少，着法可能会变差）。这种方法影响了局面的评估，相当于完全放弃了局面评估。  
方案二：返回黑白棋子之差。只能这么处理，这就需要在结果中判断一下输赢。如果子力差没变，这就说明和棋。  
当考察SearchAI对当前局面的观点时：如果局面估值为极大值，则必胜；若为极小值，则必败；否则，和棋。就算黑棋比白棋多一子，也依然是和棋。

### 如果minSonValue==fatherValue，这时应不应该进行剪枝呢？  
如果说剪枝，那是对的，因为当前节点肯定无法给出更好的答案，直接让这个节点返回MAX_VALUE才叫剪枝，不能返回-minSonValue，那样会让父节点以为这个儿子是最优秀的儿子之一，而实际上，这个节点是被剪枝了的结点，它完全有可能给出更差的解，也就是它会产生比minSonVlaue更小的解。 
如果说不剪枝，那也是对的，因为当前节点如果不剪枝，它的minSonValue有可能最终也还是fatherValue，那么这个节点必然应该作为父节点的最优儿子对待，而不应该剪掉。 
正确观点：如果不剪枝目的是增多可选解，而实际上可选解只在depth=0的情况下有意义，其它情况下根本不需要有很多可选解，所以在depth=1的情况下，即便minSonValue等于fatherValue也不能剪枝。在depth>0的情况下，只要是`minSonValue<=fatherValue`就要进行剪枝。  

### SearchAI明明必胜，却走起了重复棋
这是因为使用了一个列表存储搜索过的状态，A态为必胜态，B态也为必胜态，当搜索过A之后，A的估值变成必胜，当B进行搜索时，就必然选择了必胜态A。这样这两个状态确实都是必胜态，但是它俩就在这里来回走，似乎必胜态是一个循环。  
这个道理多么深刻呀！状态A是必胜态，状态B也是必胜态，他们互为决策。于是形成死循环，它俩从此都沦为重复态，必胜态成了一句空话，一个虚构的理想，一个可望而不可即的梦，而它俩却满怀憧憬，自以为自己是必胜态，殊不知当他俩互相依赖为必胜态的那一瞬间，它俩已经都不再是必胜态了。  
解决方法：将解与所需步数联系起来，当有多个可行解时，优先返回到达胜利需要最少步数的那个解。也就是说，评估一个结点的好坏，应该考虑这个结点到达游戏结束的距离。
